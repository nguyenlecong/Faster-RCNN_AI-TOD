{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/yolov5/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules import utils\n",
    "from modules import transforms as T\n",
    "from modules.engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cython\n",
    "# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/pytorch/vision.git\n",
    "\n",
    "# !cp vision/references/detection/utils.py .\n",
    "# !cp vision/references/detection/transforms.py .\n",
    "# !cp vision/references/detection/coco_eval.py .\n",
    "# !cp vision/references/detection/engine.py .\n",
    "# !cp vision/references/detection/coco_utils.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = config.min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = config.class_map\n",
    "num_classes = len(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = torchvision.models.vgg16(\n",
    "                    weights=\"VGG16_Weights.IMAGENET1K_V1\").features\n",
    "backbone.out_channels = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_size = config.anchor_size\n",
    "anchor_ratio = config.anchor_ratio\n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=(anchor_size,),\n",
    "                                   aspect_ratios=(anchor_ratio,))\n",
    "\n",
    "# if rpn_anchor_generator is None:\n",
    "#     rpn_anchor_generator = _default_anchorgen()\n",
    "\n",
    "# def _default_anchorgen():\n",
    "#     anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "#     aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "#     return AnchorGenerator(anchor_sizes, aspect_ratios)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# if box_roi_pool is None:\n",
    "#     box_roi_pool = MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FasterRCNN(backbone=backbone,\n",
    "                   num_classes=num_classes,\n",
    "                   min_size=min_size,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(600,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 84, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=8, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# model.eval()\n",
    "# x = [torch.rand(3, 300, 400)]\n",
    "# predictions = model(x)\n",
    "\n",
    "# make_dot(predictions[0]['boxes'], params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))  # Follow Fast R-CNN paper\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.label_root = root.replace('/hdd/thaihq/qnet_search/ori_data', '/hdd/nguyenlc/ai_training/faster_rcnn/qnet_search/correct_labels')\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        # self.labels = list(sorted(os.listdir(os.path.join(self.label_root, \"labels\"))))\n",
    "        self.labels = list(sorted(os.listdir(self.label_root)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        label_path = os.path.join(self.label_root, self.labels[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(label_path) as file:\n",
    "            label = [line.rstrip() for line in file]\n",
    "\n",
    "        num_objs = len(label)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in range(num_objs):\n",
    "            # xmin, ymin, xmax, ymax, cls = [int(j) for j in label[i].split(', ')]\n",
    "            # boxes.append([xmin, ymin, xmax, ymax])\n",
    "            xmin, ymin, width, height, cls = [int(j) for j in label[i].split(', ')]\n",
    "            boxes.append([xmin, ymin, xmin+width, ymin+height])\n",
    "            labels.append(cls-1)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = config.train_ratio\n",
    "# val_ratio = config.val_ratio\n",
    "# train_val_batch = config.train_val_batch\n",
    "# test_batch = config.test_batch\n",
    "\n",
    "# train_dataset = CustomDataset(root, get_transform(train=True))\n",
    "# val_test_dataset = CustomDataset(root, get_transform(train=False))\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "# train_num = round(train_ratio*len(train_dataset))\n",
    "# val_num = round(val_ratio*len(train_dataset))\n",
    "# indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, indices[:train_num])\n",
    "# val_dataset = torch.utils.data.Subset(val_test_dataset, indices[train_num:train_num+val_num])\n",
    "# test_dataset = torch.utils.data.Subset(val_test_dataset, indices[train_num+val_num:])\n",
    "\n",
    "# train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_val_batch,\n",
    "#                                           shuffle=True, num_workers=4,\n",
    "#                                           collate_fn=utils.collate_fn)\n",
    "\n",
    "# val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=train_val_batch,\n",
    "#                                                shuffle=False, num_workers=4,\n",
    "#                                                collate_fn=utils.collate_fn)\n",
    "\n",
    "# len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11214, 2804, 14018)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_batch = config.train_val_batch\n",
    "test_batch = config.test_batch\n",
    "\n",
    "train_dataset = CustomDataset('/hdd/thaihq/qnet_search/ori_data/train', get_transform(train=True))\n",
    "val_dataset = CustomDataset('/hdd/thaihq/qnet_search/ori_data/val', get_transform(train=False))\n",
    "test_dataset = CustomDataset('/hdd/thaihq/qnet_search/ori_data/test', get_transform(train=False))\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_val_batch,\n",
    "                                          shuffle=True, num_workers=4,\n",
    "                                          collate_fn=utils.collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=train_val_batch,\n",
    "                                               shuffle=False, num_workers=4,\n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=train_val_batch,\n",
    "                                               shuffle=False, num_workers=4,\n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mode = config.mode=='train'\n",
    "train_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer - SGD follow Faster R-CNN paper\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # and a learning rate scheduler which decreases the learning rate by\n",
    "    # 10x every 3 epochs\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/701]  eta: 0:33:43  lr: 0.000012  loss: 2.9930 (2.9930)  loss_classifier: 2.1357 (2.1357)  loss_box_reg: 0.0056 (0.0056)  loss_objectness: 0.6945 (0.6945)  loss_rpn_box_reg: 0.1571 (0.1571)  time: 2.8863  data: 0.6259  max mem: 10287\n",
      "Epoch: [0]  [ 10/701]  eta: 0:09:11  lr: 0.000083  loss: 2.9961 (3.0107)  loss_classifier: 2.1282 (2.1278)  loss_box_reg: 0.0038 (0.0042)  loss_objectness: 0.6937 (0.6938)  loss_rpn_box_reg: 0.1606 (0.1849)  time: 0.7976  data: 0.0770  max mem: 10623\n",
      "Epoch: [0]  [ 20/701]  eta: 0:07:55  lr: 0.000155  loss: 2.9879 (2.9918)  loss_classifier: 2.1095 (2.1035)  loss_box_reg: 0.0038 (0.0043)  loss_objectness: 0.6936 (0.6937)  loss_rpn_box_reg: 0.1920 (0.1903)  time: 0.5893  data: 0.0222  max mem: 10623\n",
      "Epoch: [0]  [ 30/701]  eta: 0:07:25  lr: 0.000226  loss: 2.9190 (2.9383)  loss_classifier: 2.0283 (2.0564)  loss_box_reg: 0.0033 (0.0041)  loss_objectness: 0.6930 (0.6933)  loss_rpn_box_reg: 0.1920 (0.1846)  time: 0.5913  data: 0.0220  max mem: 10623\n",
      "Epoch: [0]  [ 40/701]  eta: 0:07:07  lr: 0.000298  loss: 2.7021 (2.8622)  loss_classifier: 1.8856 (1.9802)  loss_box_reg: 0.0033 (0.0041)  loss_objectness: 0.6915 (0.6926)  loss_rpn_box_reg: 0.1834 (0.1853)  time: 0.5913  data: 0.0207  max mem: 10623\n",
      "Epoch: [0]  [ 50/701]  eta: 0:06:54  lr: 0.000369  loss: 2.4255 (2.7235)  loss_classifier: 1.5817 (1.8442)  loss_box_reg: 0.0045 (0.0046)  loss_objectness: 0.6892 (0.6916)  loss_rpn_box_reg: 0.1789 (0.1831)  time: 0.5919  data: 0.0212  max mem: 10623\n",
      "Epoch: [0]  [ 60/701]  eta: 0:06:43  lr: 0.000440  loss: 1.6447 (2.5283)  loss_classifier: 0.8236 (1.6569)  loss_box_reg: 0.0078 (0.0055)  loss_objectness: 0.6819 (0.6890)  loss_rpn_box_reg: 0.1520 (0.1769)  time: 0.5973  data: 0.0233  max mem: 10623\n",
      "Epoch: [0]  [ 70/701]  eta: 0:06:34  lr: 0.000512  loss: 1.4572 (2.3485)  loss_classifier: 0.5789 (1.4846)  loss_box_reg: 0.0112 (0.0066)  loss_objectness: 0.6650 (0.6829)  loss_rpn_box_reg: 0.1520 (0.1744)  time: 0.5995  data: 0.0223  max mem: 10623\n",
      "Epoch: [0]  [ 80/701]  eta: 0:06:27  lr: 0.000583  loss: 1.1123 (2.1830)  loss_classifier: 0.3042 (1.3306)  loss_box_reg: 0.0105 (0.0068)  loss_objectness: 0.6343 (0.6744)  loss_rpn_box_reg: 0.1556 (0.1712)  time: 0.6038  data: 0.0231  max mem: 10623\n",
      "Epoch: [0]  [ 90/701]  eta: 0:06:20  lr: 0.000654  loss: 0.9328 (2.0462)  loss_classifier: 0.2156 (1.2120)  loss_box_reg: 0.0104 (0.0078)  loss_objectness: 0.5818 (0.6553)  loss_rpn_box_reg: 0.1527 (0.1711)  time: 0.6144  data: 0.0248  max mem: 10623\n",
      "Epoch: [0]  [100/701]  eta: 0:06:13  lr: 0.000726  loss: 0.8143 (1.9121)  loss_classifier: 0.2123 (1.1132)  loss_box_reg: 0.0150 (0.0086)  loss_objectness: 0.3697 (0.6188)  loss_rpn_box_reg: 0.1527 (0.1715)  time: 0.6114  data: 0.0232  max mem: 10623\n",
      "Epoch: [0]  [110/701]  eta: 0:06:06  lr: 0.000797  loss: 0.5466 (1.7831)  loss_classifier: 0.1804 (1.0277)  loss_box_reg: 0.0131 (0.0088)  loss_objectness: 0.2419 (0.5825)  loss_rpn_box_reg: 0.1137 (0.1641)  time: 0.6017  data: 0.0216  max mem: 10623\n",
      "Epoch: [0]  [120/701]  eta: 0:05:59  lr: 0.000868  loss: 0.4848 (1.6766)  loss_classifier: 0.1615 (0.9571)  loss_box_reg: 0.0130 (0.0092)  loss_objectness: 0.2079 (0.5501)  loss_rpn_box_reg: 0.0970 (0.1602)  time: 0.6025  data: 0.0201  max mem: 10623\n",
      "Epoch: [0]  [130/701]  eta: 0:05:52  lr: 0.000940  loss: 0.4616 (1.5844)  loss_classifier: 0.1540 (0.8964)  loss_box_reg: 0.0145 (0.0097)  loss_objectness: 0.1836 (0.5215)  loss_rpn_box_reg: 0.1061 (0.1568)  time: 0.6065  data: 0.0212  max mem: 10623\n",
      "Epoch: [0]  [140/701]  eta: 0:05:45  lr: 0.001011  loss: 0.4569 (1.5043)  loss_classifier: 0.1505 (0.8438)  loss_box_reg: 0.0126 (0.0099)  loss_objectness: 0.1798 (0.4966)  loss_rpn_box_reg: 0.1061 (0.1539)  time: 0.6057  data: 0.0216  max mem: 10623\n",
      "Epoch: [0]  [150/701]  eta: 0:05:39  lr: 0.001082  loss: 0.3864 (1.4272)  loss_classifier: 0.1315 (0.7959)  loss_box_reg: 0.0120 (0.0101)  loss_objectness: 0.1430 (0.4724)  loss_rpn_box_reg: 0.0878 (0.1489)  time: 0.6019  data: 0.0198  max mem: 10623\n",
      "Epoch: [0]  [160/701]  eta: 0:05:32  lr: 0.001154  loss: 0.3764 (1.3643)  loss_classifier: 0.1219 (0.7558)  loss_box_reg: 0.0139 (0.0105)  loss_objectness: 0.1351 (0.4519)  loss_rpn_box_reg: 0.0944 (0.1462)  time: 0.6024  data: 0.0203  max mem: 10623\n",
      "Epoch: [0]  [170/701]  eta: 0:05:26  lr: 0.001225  loss: 0.3941 (1.3064)  loss_classifier: 0.1308 (0.7194)  loss_box_reg: 0.0157 (0.0107)  loss_objectness: 0.1312 (0.4334)  loss_rpn_box_reg: 0.0984 (0.1428)  time: 0.6072  data: 0.0223  max mem: 10623\n",
      "Epoch: [0]  [180/701]  eta: 0:05:21  lr: 0.001297  loss: 0.3342 (1.2552)  loss_classifier: 0.1282 (0.6875)  loss_box_reg: 0.0137 (0.0109)  loss_objectness: 0.1272 (0.4167)  loss_rpn_box_reg: 0.0804 (0.1402)  time: 0.6370  data: 0.0256  max mem: 11056\n",
      "Epoch: [0]  [190/701]  eta: 0:05:15  lr: 0.001368  loss: 0.2971 (1.2043)  loss_classifier: 0.1104 (0.6567)  loss_box_reg: 0.0126 (0.0108)  loss_objectness: 0.1074 (0.4006)  loss_rpn_box_reg: 0.0667 (0.1362)  time: 0.6427  data: 0.0268  max mem: 11056\n",
      "Epoch: [0]  [200/701]  eta: 0:05:08  lr: 0.001439  loss: 0.2692 (1.1599)  loss_classifier: 0.0952 (0.6295)  loss_box_reg: 0.0126 (0.0111)  loss_objectness: 0.0995 (0.3860)  loss_rpn_box_reg: 0.0629 (0.1333)  time: 0.6122  data: 0.0228  max mem: 11056\n",
      "Epoch: [0]  [210/701]  eta: 0:05:02  lr: 0.001511  loss: 0.2813 (1.1230)  loss_classifier: 0.1053 (0.6060)  loss_box_reg: 0.0161 (0.0117)  loss_objectness: 0.1079 (0.3735)  loss_rpn_box_reg: 0.0667 (0.1319)  time: 0.6045  data: 0.0203  max mem: 11056\n",
      "Epoch: [0]  [220/701]  eta: 0:04:56  lr: 0.001582  loss: 0.3864 (1.0904)  loss_classifier: 0.1476 (0.5851)  loss_box_reg: 0.0198 (0.0125)  loss_objectness: 0.1249 (0.3623)  loss_rpn_box_reg: 0.0990 (0.1305)  time: 0.6070  data: 0.0211  max mem: 11056\n",
      "Epoch: [0]  [230/701]  eta: 0:04:49  lr: 0.001653  loss: 0.3626 (1.0574)  loss_classifier: 0.1182 (0.5649)  loss_box_reg: 0.0198 (0.0130)  loss_objectness: 0.1155 (0.3512)  loss_rpn_box_reg: 0.0848 (0.1284)  time: 0.6094  data: 0.0221  max mem: 11056\n",
      "Epoch: [0]  [240/701]  eta: 0:04:43  lr: 0.001725  loss: 0.3675 (1.0281)  loss_classifier: 0.1182 (0.5468)  loss_box_reg: 0.0177 (0.0133)  loss_objectness: 0.1046 (0.3410)  loss_rpn_box_reg: 0.0925 (0.1269)  time: 0.6129  data: 0.0248  max mem: 11056\n",
      "Epoch: [0]  [250/701]  eta: 0:04:37  lr: 0.001796  loss: 0.2718 (0.9985)  loss_classifier: 0.1102 (0.5294)  loss_box_reg: 0.0148 (0.0135)  loss_objectness: 0.0981 (0.3311)  loss_rpn_box_reg: 0.0729 (0.1245)  time: 0.6111  data: 0.0247  max mem: 11056\n",
      "Epoch: [0]  [260/701]  eta: 0:04:31  lr: 0.001867  loss: 0.3143 (0.9731)  loss_classifier: 0.1215 (0.5141)  loss_box_reg: 0.0148 (0.0137)  loss_objectness: 0.0943 (0.3225)  loss_rpn_box_reg: 0.0729 (0.1228)  time: 0.6077  data: 0.0214  max mem: 11056\n",
      "Epoch: [0]  [270/701]  eta: 0:04:25  lr: 0.001939  loss: 0.3143 (0.9486)  loss_classifier: 0.1215 (0.4996)  loss_box_reg: 0.0133 (0.0138)  loss_objectness: 0.1009 (0.3143)  loss_rpn_box_reg: 0.0726 (0.1210)  time: 0.6116  data: 0.0223  max mem: 11056\n",
      "Epoch: [0]  [280/701]  eta: 0:04:19  lr: 0.002010  loss: 0.3281 (0.9278)  loss_classifier: 0.1227 (0.4869)  loss_box_reg: 0.0142 (0.0139)  loss_objectness: 0.1119 (0.3070)  loss_rpn_box_reg: 0.0723 (0.1200)  time: 0.6258  data: 0.0266  max mem: 11056\n",
      "Epoch: [0]  [290/701]  eta: 0:04:13  lr: 0.002081  loss: 0.3644 (0.9085)  loss_classifier: 0.1455 (0.4749)  loss_box_reg: 0.0207 (0.0142)  loss_objectness: 0.1119 (0.3002)  loss_rpn_box_reg: 0.0942 (0.1192)  time: 0.6271  data: 0.0272  max mem: 11056\n",
      "Epoch: [0]  [300/701]  eta: 0:04:06  lr: 0.002153  loss: 0.3375 (0.8888)  loss_classifier: 0.1250 (0.4631)  loss_box_reg: 0.0190 (0.0143)  loss_objectness: 0.1051 (0.2935)  loss_rpn_box_reg: 0.0880 (0.1179)  time: 0.6135  data: 0.0229  max mem: 11056\n",
      "Epoch: [0]  [310/701]  eta: 0:04:00  lr: 0.002224  loss: 0.3297 (0.8713)  loss_classifier: 0.1250 (0.4526)  loss_box_reg: 0.0136 (0.0143)  loss_objectness: 0.0980 (0.2875)  loss_rpn_box_reg: 0.0758 (0.1169)  time: 0.6060  data: 0.0191  max mem: 11056\n",
      "Epoch: [0]  [320/701]  eta: 0:03:54  lr: 0.002296  loss: 0.3271 (0.8542)  loss_classifier: 0.1188 (0.4424)  loss_box_reg: 0.0137 (0.0144)  loss_objectness: 0.0980 (0.2818)  loss_rpn_box_reg: 0.0752 (0.1157)  time: 0.6137  data: 0.0219  max mem: 11056\n",
      "Epoch: [0]  [330/701]  eta: 0:03:48  lr: 0.002367  loss: 0.3392 (0.8391)  loss_classifier: 0.1251 (0.4333)  loss_box_reg: 0.0202 (0.0146)  loss_objectness: 0.1011 (0.2763)  loss_rpn_box_reg: 0.0879 (0.1150)  time: 0.6198  data: 0.0251  max mem: 11056\n",
      "Epoch: [0]  [340/701]  eta: 0:03:42  lr: 0.002438  loss: 0.3444 (0.8242)  loss_classifier: 0.1280 (0.4245)  loss_box_reg: 0.0178 (0.0147)  loss_objectness: 0.1060 (0.2711)  loss_rpn_box_reg: 0.0879 (0.1139)  time: 0.6119  data: 0.0227  max mem: 11056\n",
      "Epoch: [0]  [350/701]  eta: 0:03:36  lr: 0.002510  loss: 0.2883 (0.8088)  loss_classifier: 0.1085 (0.4157)  loss_box_reg: 0.0133 (0.0147)  loss_objectness: 0.0887 (0.2658)  loss_rpn_box_reg: 0.0702 (0.1126)  time: 0.6161  data: 0.0240  max mem: 11056\n",
      "Epoch: [0]  [360/701]  eta: 0:03:29  lr: 0.002581  loss: 0.2883 (0.7954)  loss_classifier: 0.1085 (0.4078)  loss_box_reg: 0.0166 (0.0148)  loss_objectness: 0.0887 (0.2610)  loss_rpn_box_reg: 0.0756 (0.1117)  time: 0.6174  data: 0.0243  max mem: 11056\n",
      "Epoch: [0]  [370/701]  eta: 0:03:23  lr: 0.002652  loss: 0.3297 (0.7833)  loss_classifier: 0.1244 (0.4006)  loss_box_reg: 0.0182 (0.0149)  loss_objectness: 0.0931 (0.2566)  loss_rpn_box_reg: 0.0771 (0.1111)  time: 0.6208  data: 0.0254  max mem: 11056\n",
      "Epoch: [0]  [380/701]  eta: 0:03:17  lr: 0.002724  loss: 0.3269 (0.7727)  loss_classifier: 0.1348 (0.3940)  loss_box_reg: 0.0182 (0.0151)  loss_objectness: 0.0873 (0.2526)  loss_rpn_box_reg: 0.0915 (0.1109)  time: 0.6194  data: 0.0252  max mem: 11056\n",
      "Epoch: [0]  [390/701]  eta: 0:03:11  lr: 0.002795  loss: 0.3063 (0.7602)  loss_classifier: 0.1180 (0.3868)  loss_box_reg: 0.0139 (0.0152)  loss_objectness: 0.0873 (0.2483)  loss_rpn_box_reg: 0.0751 (0.1099)  time: 0.6066  data: 0.0201  max mem: 11056\n",
      "Epoch: [0]  [400/701]  eta: 0:03:05  lr: 0.002866  loss: 0.2592 (0.7484)  loss_classifier: 0.1125 (0.3801)  loss_box_reg: 0.0139 (0.0153)  loss_objectness: 0.0754 (0.2441)  loss_rpn_box_reg: 0.0639 (0.1089)  time: 0.6082  data: 0.0212  max mem: 11056\n",
      "Epoch: [0]  [410/701]  eta: 0:02:58  lr: 0.002938  loss: 0.2592 (0.7380)  loss_classifier: 0.1104 (0.3739)  loss_box_reg: 0.0208 (0.0155)  loss_objectness: 0.0793 (0.2403)  loss_rpn_box_reg: 0.0667 (0.1082)  time: 0.6088  data: 0.0226  max mem: 11056\n",
      "Epoch: [0]  [420/701]  eta: 0:02:52  lr: 0.003009  loss: 0.3430 (0.7290)  loss_classifier: 0.1369 (0.3687)  loss_box_reg: 0.0251 (0.0158)  loss_objectness: 0.0927 (0.2369)  loss_rpn_box_reg: 0.0811 (0.1077)  time: 0.6091  data: 0.0224  max mem: 11056\n",
      "Epoch: [0]  [430/701]  eta: 0:02:46  lr: 0.003080  loss: 0.3494 (0.7214)  loss_classifier: 0.1434 (0.3640)  loss_box_reg: 0.0253 (0.0160)  loss_objectness: 0.0927 (0.2337)  loss_rpn_box_reg: 0.0880 (0.1077)  time: 0.6114  data: 0.0238  max mem: 11056\n",
      "Epoch: [0]  [440/701]  eta: 0:02:40  lr: 0.003152  loss: 0.3262 (0.7116)  loss_classifier: 0.1323 (0.3585)  loss_box_reg: 0.0181 (0.0160)  loss_objectness: 0.0846 (0.2302)  loss_rpn_box_reg: 0.0783 (0.1068)  time: 0.6092  data: 0.0226  max mem: 11056\n",
      "Epoch: [0]  [450/701]  eta: 0:02:34  lr: 0.003223  loss: 0.2527 (0.7019)  loss_classifier: 0.1071 (0.3531)  loss_box_reg: 0.0149 (0.0161)  loss_objectness: 0.0817 (0.2269)  loss_rpn_box_reg: 0.0590 (0.1059)  time: 0.6073  data: 0.0213  max mem: 11056\n",
      "Epoch: [0]  [460/701]  eta: 0:02:28  lr: 0.003295  loss: 0.2763 (0.6940)  loss_classifier: 0.1127 (0.3484)  loss_box_reg: 0.0184 (0.0162)  loss_objectness: 0.0831 (0.2239)  loss_rpn_box_reg: 0.0603 (0.1054)  time: 0.6089  data: 0.0220  max mem: 11056\n",
      "Epoch: [0]  [470/701]  eta: 0:02:21  lr: 0.003366  loss: 0.3673 (0.6856)  loss_classifier: 0.1444 (0.3436)  loss_box_reg: 0.0196 (0.0163)  loss_objectness: 0.0906 (0.2209)  loss_rpn_box_reg: 0.0900 (0.1048)  time: 0.6087  data: 0.0219  max mem: 11056\n",
      "Epoch: [0]  [480/701]  eta: 0:02:15  lr: 0.003437  loss: 0.2868 (0.6780)  loss_classifier: 0.1111 (0.3390)  loss_box_reg: 0.0247 (0.0165)  loss_objectness: 0.0805 (0.2181)  loss_rpn_box_reg: 0.0632 (0.1042)  time: 0.6052  data: 0.0206  max mem: 11056\n",
      "Epoch: [0]  [490/701]  eta: 0:02:09  lr: 0.003509  loss: 0.2931 (0.6712)  loss_classifier: 0.1169 (0.3350)  loss_box_reg: 0.0247 (0.0167)  loss_objectness: 0.0828 (0.2155)  loss_rpn_box_reg: 0.0693 (0.1039)  time: 0.6090  data: 0.0222  max mem: 11056\n",
      "Epoch: [0]  [500/701]  eta: 0:02:03  lr: 0.003580  loss: 0.3074 (0.6640)  loss_classifier: 0.1278 (0.3309)  loss_box_reg: 0.0223 (0.0169)  loss_objectness: 0.0841 (0.2130)  loss_rpn_box_reg: 0.0728 (0.1033)  time: 0.6117  data: 0.0233  max mem: 11056\n",
      "Epoch: [0]  [510/701]  eta: 0:01:57  lr: 0.003651  loss: 0.3362 (0.6583)  loss_classifier: 0.1348 (0.3273)  loss_box_reg: 0.0244 (0.0171)  loss_objectness: 0.0938 (0.2107)  loss_rpn_box_reg: 0.0805 (0.1031)  time: 0.6120  data: 0.0227  max mem: 11056\n",
      "Epoch: [0]  [520/701]  eta: 0:01:51  lr: 0.003723  loss: 0.2921 (0.6508)  loss_classifier: 0.1227 (0.3232)  loss_box_reg: 0.0203 (0.0172)  loss_objectness: 0.0857 (0.2081)  loss_rpn_box_reg: 0.0750 (0.1023)  time: 0.6119  data: 0.0231  max mem: 11056\n",
      "Epoch: [0]  [530/701]  eta: 0:01:44  lr: 0.003794  loss: 0.2806 (0.6448)  loss_classifier: 0.1148 (0.3197)  loss_box_reg: 0.0193 (0.0173)  loss_objectness: 0.0740 (0.2058)  loss_rpn_box_reg: 0.0652 (0.1020)  time: 0.6080  data: 0.0219  max mem: 11056\n",
      "Epoch: [0]  [540/701]  eta: 0:01:38  lr: 0.003865  loss: 0.3095 (0.6394)  loss_classifier: 0.1271 (0.3164)  loss_box_reg: 0.0251 (0.0175)  loss_objectness: 0.0808 (0.2037)  loss_rpn_box_reg: 0.0795 (0.1018)  time: 0.6082  data: 0.0210  max mem: 11056\n",
      "Epoch: [0]  [550/701]  eta: 0:01:32  lr: 0.003937  loss: 0.3027 (0.6327)  loss_classifier: 0.1245 (0.3127)  loss_box_reg: 0.0229 (0.0175)  loss_objectness: 0.0815 (0.2013)  loss_rpn_box_reg: 0.0698 (0.1011)  time: 0.6168  data: 0.0240  max mem: 11056\n",
      "Epoch: [0]  [560/701]  eta: 0:01:26  lr: 0.004008  loss: 0.2667 (0.6264)  loss_classifier: 0.1147 (0.3092)  loss_box_reg: 0.0194 (0.0175)  loss_objectness: 0.0751 (0.1991)  loss_rpn_box_reg: 0.0609 (0.1005)  time: 0.6280  data: 0.0278  max mem: 11056\n",
      "Epoch: [0]  [570/701]  eta: 0:01:20  lr: 0.004079  loss: 0.2469 (0.6205)  loss_classifier: 0.0970 (0.3059)  loss_box_reg: 0.0166 (0.0176)  loss_objectness: 0.0744 (0.1970)  loss_rpn_box_reg: 0.0550 (0.1000)  time: 0.6218  data: 0.0254  max mem: 11056\n",
      "Epoch: [0]  [580/701]  eta: 0:01:14  lr: 0.004151  loss: 0.2469 (0.6146)  loss_classifier: 0.0970 (0.3027)  loss_box_reg: 0.0166 (0.0176)  loss_objectness: 0.0754 (0.1948)  loss_rpn_box_reg: 0.0600 (0.0994)  time: 0.6078  data: 0.0213  max mem: 11056\n",
      "Epoch: [0]  [590/701]  eta: 0:01:08  lr: 0.004222  loss: 0.2225 (0.6087)  loss_classifier: 0.0938 (0.2994)  loss_box_reg: 0.0198 (0.0177)  loss_objectness: 0.0676 (0.1928)  loss_rpn_box_reg: 0.0600 (0.0988)  time: 0.6042  data: 0.0198  max mem: 11056\n"
     ]
    }
   ],
   "source": [
    "if train_mode:\n",
    "    num_epochs = config.num_epochs\n",
    "\n",
    "    logs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        log = train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=10)\n",
    "        logs.append(log)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the test dataset\n",
    "        # evaluate(model, val_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss):\n",
    "    loss_1 = [float(str(i).split('  ')[1].split(' ')[1]) for i in loss]\n",
    "    loss_2 = [float(str(i).split('  ')[1].split(' ')[2][1:-1]) for i in loss]\n",
    "\n",
    "    plt.figure(figsize=(15, 10), tight_layout=True)\n",
    "    plt.plot(range(len(loss_1)), loss_1, label='loss_1')\n",
    "    plt.plot(range(len(loss_2)), loss_2, label='loss_2')\n",
    "\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'loss_train.png')\n",
    "    plt.show()\n",
    "\n",
    "plot(logs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    file_num = len([i for i in os.listdir('weights')])\n",
    "    torch.save(model, f'weights/exp_{file_num}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(model, test_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import color_palette\n",
    "\n",
    "\n",
    "def make_color_map():\n",
    "    '''\n",
    "        Create a color map for each class\n",
    "    '''\n",
    "    names = sorted(set(list(class_map.keys())))\n",
    "    n = len(names)\n",
    "    cp = color_palette(\"Paired\", n)\n",
    "\n",
    "    cp[:] = [tuple(int(255*c) for c in rgb) for rgb in cp]\n",
    "\n",
    "    return dict(zip(names, cp))\n",
    "\n",
    "if not train_mode:   \n",
    "    color_map = make_color_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, iou_thresh=0.5):\n",
    "    return torchvision.ops.nms(boxes, scores, iou_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(image, boxes, cls, scr):\n",
    "    color = color_map[cls]\n",
    "\n",
    "    x1, y1, x2, y2 = boxes.cpu().numpy().astype(\"int\")\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 1)\n",
    "\n",
    "    cv2.putText(image, ' '.join([cls, scr]), (x1, y1-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1,\n",
    "                cv2.LINE_AA)\n",
    "    # cv2.putText(image, ' '.join([cls, scr]), (x1, y1-10),\n",
    "    #             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 1,\n",
    "    #             cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, img):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model([img.to(device)])\n",
    "\n",
    "    image = img.numpy().transpose(1, 2, 0)\n",
    "    image = (image*255).astype('uint8')\n",
    "\n",
    "    boxes = pred[0]['boxes']\n",
    "    scores = pred[0]['scores']\n",
    "    keep = nms(boxes, scores, 0.1)\n",
    "\n",
    "    labels = pred[0]['labels']\n",
    "\n",
    "    for i in keep:\n",
    "        # cls = class_map[int(labels[i])]\n",
    "        cls = list(class_map.keys())[int(labels[i])]\n",
    "        scr = str(round(float(scores[i]), 2))\n",
    "        plot_result(image, boxes[i], cls, scr)\n",
    "\n",
    "    image = image[:,:,::-1]\n",
    "    cv2.imwrite('tmp.tif', image)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(image[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # pick one image from the test set\n",
    "    img, _ = test_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # load model from weight\n",
    "    # model1 = torch.load('weights/exp.pt')\n",
    "    # model2 = torch.load('weights/exp_1.pt')\n",
    "    # model3 = torch.load('weights/exp_2.pt')\n",
    "    model4 = torch.load('weights/exp_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not train_mode:\n",
    "#     # AnchorGenerator(sizes=((64, 128, 256, 512),),\n",
    "#     #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "#     # min_size = 600\n",
    "#     infer(model1, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not train_mode:\n",
    "#     # AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#     #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "#     # min_size = 1200\n",
    "#     infer(model2, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not train_mode:\n",
    "#     # AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n",
    "#     #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "#     # min_size = 1200\n",
    "#     infer(model3, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n",
    "    #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    # min_size = 600\n",
    "    infer(model4, img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd481019de055d53831c4816760b4099e1ec54cf808b21949a4b525a6c74a9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
