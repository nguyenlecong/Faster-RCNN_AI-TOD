{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/yolov5/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules import utils\n",
    "from modules import transforms as T\n",
    "from modules.engine import train_one_epoch, evaluate\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cython\n",
    "# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "\n",
    "# !git clone https://github.com/pytorch/vision.git\n",
    "\n",
    "# !cp vision/references/detection/utils.py .\n",
    "# !cp vision/references/detection/transforms.py .\n",
    "# !cp vision/references/detection/coco_eval.py .\n",
    "# !cp vision/references/detection/engine.py .\n",
    "# !cp vision/references/detection/coco_utils.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(600,), max_size=800, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(512, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=8, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone = torchvision.models.vgg16(weights=\"VGG16_Weights.IMAGENET1K_V1\").features\n",
    "backbone.out_channels = 512\n",
    "\n",
    "anchor_size = config.anchor_size\n",
    "anchor_ratio = config.anchor_ratio\n",
    "anchor_generator = AnchorGenerator(sizes=(anchor_size,),\n",
    "                                   aspect_ratios=(anchor_ratio,))\n",
    "\n",
    "class_map = config.class_map\n",
    "num_classes = len(class_map)\n",
    "\n",
    "min_size = config.min_size\n",
    "max_size = config.max_size\n",
    "\n",
    "model = FasterRCNN(backbone=backbone,\n",
    "                   num_classes=num_classes,\n",
    "                   min_size=min_size,\n",
    "                   max_size=max_size,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   )\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))  # Follow Fast R-CNN paper\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.label_root = root.replace('/hdd/thaihq/qnet_search/ori_data', '/hdd/nguyenlc/ai_training/faster_rcnn/qnet_search/correct_labels')\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        # self.labels = list(sorted(os.listdir(os.path.join(self.label_root, \"labels\"))))\n",
    "        self.labels = list(sorted(os.listdir(self.label_root)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        label_path = os.path.join(self.label_root, self.labels[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(label_path) as file:\n",
    "            label = [line.rstrip() for line in file]\n",
    "\n",
    "        num_objs = len(label)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in range(num_objs):\n",
    "            # xmin, ymin, xmax, ymax, cls = [int(j) for j in label[i].split(', ')]\n",
    "            # boxes.append([xmin, ymin, xmax, ymax])\n",
    "            xmin, ymin, width, height, cls = [int(j) for j in label[i].split(', ')]\n",
    "            boxes.append([xmin, ymin, xmin+width, ymin+height])\n",
    "            labels.append(cls-1)  #**********\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        target[\"image_name\"] = os.path.basename(img_path).split('.')[0]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = config.train_ratio\n",
    "# val_ratio = config.val_ratio\n",
    "# train_val_batch = config.train_val_batch\n",
    "# test_batch = config.test_batch\n",
    "\n",
    "# train_dataset = CustomDataset(root, get_transform(train=True))\n",
    "# val_test_dataset = CustomDataset(root, get_transform(train=False))\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "# train_num = round(train_ratio*len(train_dataset))\n",
    "# val_num = round(val_ratio*len(train_dataset))\n",
    "# indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, indices[:train_num])\n",
    "# val_dataset = torch.utils.data.Subset(val_test_dataset, indices[train_num:train_num+val_num])\n",
    "# test_dataset = torch.utils.data.Subset(val_test_dataset, indices[train_num+val_num:])\n",
    "\n",
    "# train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_val_batch,\n",
    "#                                           shuffle=True, num_workers=4,\n",
    "#                                           collate_fn=utils.collate_fn)\n",
    "\n",
    "# val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=train_val_batch,\n",
    "#                                                shuffle=False, num_workers=4,\n",
    "#                                                collate_fn=utils.collate_fn)\n",
    "\n",
    "# len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11214, 2804, 14018)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch = config.train_batch\n",
    "val_test_batch = config.val_test_batch\n",
    "\n",
    "train_dataset = CustomDataset('/hdd/thaihq/qnet_search/ori_data/train', get_transform(train=True))\n",
    "val_dataset = CustomDataset('/hdd/thaihq/qnet_search/ori_data/val', get_transform(train=False))\n",
    "test_dataset = CustomDataset('/hdd/thaihq/qnet_search/ori_data/test', get_transform(train=False))\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch,\n",
    "                                          shuffle=True, num_workers=4,\n",
    "                                          collate_fn=utils.collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_test_batch,\n",
    "                                               shuffle=False, num_workers=4,\n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=val_test_batch,\n",
    "                                               shuffle=False, num_workers=4,\n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mode = config.mode=='train'\n",
    "\n",
    "def create_folder(root, train_mode):\n",
    "    max_model = 0\n",
    "    for root, j, _ in os.walk(root):\n",
    "        for dirs in j:\n",
    "            try:\n",
    "                temp = int(dirs)\n",
    "                if temp > max_model:\n",
    "                    max_model = temp\n",
    "            except:\n",
    "                continue\n",
    "        break\n",
    "    max_model += 1\n",
    "\n",
    "    log_path = os.path.join(root, str(max_model))\n",
    "    os.makedirs(log_path)\n",
    "\n",
    "    if train_mode:\n",
    "        weight_path = os.path.join(log_path, 'weights')\n",
    "        os.makedirs(weight_path)\n",
    "    else:\n",
    "        weight_path = None\n",
    "\n",
    "    return log_path, weight_path\n",
    "\n",
    "if train_mode:\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer - SGD follow Faster R-CNN paper\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=config.learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # and a learning rate scheduler which decreases the learning rate by\n",
    "    # 10x every 3 epochs\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/1402]  eta: 1:11:39  lr: 0.000002  loss: 2.9816 (2.9816)  loss_classifier: 2.1161 (2.1161)  loss_box_reg: 0.0069 (0.0069)  loss_objectness: 0.6911 (0.6911)  loss_rpn_box_reg: 0.1675 (0.1675)  time: 3.0665  data: 1.1960  max mem: 5281\n",
      "Epoch: [0]  [ 100/1402]  eta: 0:07:38  lr: 0.000102  loss: 2.5163 (2.8316)  loss_classifier: 1.6472 (1.9411)  loss_box_reg: 0.0030 (0.0028)  loss_objectness: 0.6851 (0.6888)  loss_rpn_box_reg: 0.1910 (0.1990)  time: 0.3180  data: 0.0118  max mem: 5831\n",
      "Epoch: [0]  [ 200/1402]  eta: 0:06:47  lr: 0.000202  loss: 0.5663 (2.0064)  loss_classifier: 0.1768 (1.2327)  loss_box_reg: 0.0170 (0.0086)  loss_objectness: 0.2346 (0.5927)  loss_rpn_box_reg: 0.1287 (0.1724)  time: 0.3231  data: 0.0118  max mem: 5831\n",
      "Epoch: [0]  [ 300/1402]  eta: 0:06:10  lr: 0.000302  loss: 0.4110 (1.4781)  loss_classifier: 0.1465 (0.8727)  loss_box_reg: 0.0284 (0.0123)  loss_objectness: 0.1207 (0.4424)  loss_rpn_box_reg: 0.0921 (0.1506)  time: 0.3269  data: 0.0125  max mem: 5831\n",
      "Epoch: [0]  [ 400/1402]  eta: 0:05:33  lr: 0.000402  loss: 0.3297 (1.1975)  loss_classifier: 0.1360 (0.6894)  loss_box_reg: 0.0132 (0.0132)  loss_objectness: 0.0986 (0.3590)  loss_rpn_box_reg: 0.0869 (0.1358)  time: 0.3231  data: 0.0111  max mem: 5831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.105/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m logs \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.105/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.105/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.105/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     log \u001b[39m=\u001b[39m train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)  \u001b[39m# val_data_loader for debugging\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.105/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     log_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(log_folder, \u001b[39m'\u001b[39m\u001b[39mlog.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.105/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     stopper\u001b[39m.\u001b[39mlog(log_path, \u001b[39mstr\u001b[39m(log))\n",
      "File \u001b[0;32m/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/modules/engine.py:59\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m lr_scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 59\u001b[0m     metric_logger\u001b[39m.\u001b[39;49mupdate(loss\u001b[39m=\u001b[39;49mlosses_reduced, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mloss_dict_reduced)\n\u001b[1;32m     60\u001b[0m     metric_logger\u001b[39m.\u001b[39mupdate(lr\u001b[39m=\u001b[39moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m metric_logger\n",
      "File \u001b[0;32m/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/modules/utils.py:121\u001b[0m, in \u001b[0;36mMetricLogger.update\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 121\u001b[0m         v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    122\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(v, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m))\n\u001b[1;32m    123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeters[k]\u001b[39m.\u001b[39mupdate(v)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_mode:\n",
    "    from early_stopping import EarlyStopping\n",
    "    \n",
    "    stopper = EarlyStopping(config.patience)\n",
    "\n",
    "    log_folder, weight_folder = create_folder('training', train_mode)\n",
    "\n",
    "    num_epochs = config.num_epochs\n",
    "\n",
    "    logs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        log = train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=100)  # val_data_loader for debugging\n",
    "\n",
    "        log_path = os.path.join(log_folder, 'log.txt')\n",
    "        stopper.log(log_path, str(log))\n",
    "\n",
    "        last_weight_path = os.path.join(weight_folder, 'last.pt')\n",
    "        torch.save(model, last_weight_path)\n",
    "\n",
    "        # loss_1 = float(str(log).split('  ')[1].split(' ')[1])\n",
    "        loss_2 = float(str(log).split('  ')[1].split(' ')[2][1:-1])\n",
    "        loss = loss_2  # loss_1 for debugging\n",
    "\n",
    "        stop = stopper(epoch, loss)\n",
    "        if stop:\n",
    "            best_weight_path = os.path.join(weight_folder, 'best.pt')\n",
    "            torch.save(model, best_weight_path)\n",
    "            break\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the test dataset\n",
    "        # evaluate(model, val_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(log_folder):\n",
    "    path = os.path.join(log_folder, 'log.txt')\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    lines = file.readlines()\n",
    "\n",
    "    loss_1 = [float(str(i).split('  ')[1].split(' ')[1]) for i in lines]\n",
    "    loss_2 = [float(str(i).split('  ')[1].split(' ')[2][1:-1]) for i in lines]\n",
    "\n",
    "    plt.figure(figsize=(15, 10), tight_layout=True)\n",
    "    plt.plot(range(len(loss_1)), loss_1, label='loss_1')\n",
    "    plt.plot(range(len(loss_2)), loss_2, label='loss_2')\n",
    "\n",
    "    min_loss = min(loss_2)\n",
    "    min_index = loss_2.index(min(loss_2))\n",
    "    plt.plot(min_index, min_loss, '*', label='best value')\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "\n",
    "    save_path = os.path.join(log_folder, 'loss.png')\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if train_mode:\n",
    "    # log_folder = 'training/1'\n",
    "    plot(log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, iou_thresh=0.5):\n",
    "    return torchvision.ops.nms(boxes, scores, iou_thresh)\n",
    "\n",
    "def plot_result(image, boxes, cls, scr):\n",
    "    color = color_map[cls]\n",
    "\n",
    "    x1, y1, x2, y2 = boxes.cpu().numpy().astype(\"int\")\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 1)\n",
    "\n",
    "    cv2.putText(image, ' '.join([cls, scr]), (x1, y1-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1,\n",
    "                cv2.LINE_AA)\n",
    "    # cv2.putText(image, ' '.join([cls, scr]), (x1, y1-10),\n",
    "    #             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 1,\n",
    "    #             cv2.LINE_AA)\n",
    "\n",
    "\n",
    "from seaborn import color_palette\n",
    "\n",
    "def make_color_map():\n",
    "    '''\n",
    "        Create a color map for each class\n",
    "    '''\n",
    "    names = sorted(set(list(class_map.keys())))\n",
    "    n = len(names)\n",
    "    cp = color_palette(\"Paired\", n)\n",
    "\n",
    "    cp[:] = [tuple(int(255*c) for c in rgb) for rgb in cp]\n",
    "\n",
    "    return dict(zip(names, cp))\n",
    "\n",
    "if not train_mode:   \n",
    "    color_map = make_color_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, img):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model([img.to(device)])\n",
    "\n",
    "    image = img.numpy().transpose(1, 2, 0)\n",
    "    image = (image*255).astype('uint8')\n",
    "\n",
    "    boxes = pred[0]['boxes']\n",
    "    scores = pred[0]['scores']\n",
    "    keep = nms(boxes, scores, 0.1)\n",
    "\n",
    "    labels = pred[0]['labels']\n",
    "\n",
    "    for i in keep:\n",
    "        cls = list(class_map.keys())[int(labels[i])]\n",
    "        scr = str(round(float(scores[i]), 2))\n",
    "        plot_result(image, boxes[i], cls, scr)\n",
    "\n",
    "    image = image[:,:,::-1]\n",
    "    # cv2.imwrite('tmp.tif', image)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(image[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # pick one image from the test set\n",
    "    img, target = test_dataset[0]\n",
    "    print(target['image_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    model_1 = torch.load('training/1/weights/best.pt')\n",
    "    infer(model_1, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    model_2 = torch.load('training/2/weights/best.pt')\n",
    "    infer(model_2, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with torch.no_grad():\n",
    "        path, _ = create_folder('predictions', train_mode)\n",
    "\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.eval()\n",
    "\n",
    "        for i in tqdm(range(len(test_dataset))):\n",
    "            img, target = test_dataset[i]\n",
    "\n",
    "            img_name = target['image_name']\n",
    "            save_path = os.path.join(path, f'{img_name}.txt')\n",
    "            file = open(save_path, \"w\")\n",
    "\n",
    "            pred = model([img.to(device)])[0]\n",
    "            \n",
    "            boxes = pred['boxes']\n",
    "            scores = pred['scores']\n",
    "            labels = pred['labels']\n",
    "            \n",
    "            zip_lists = zip(labels, scores, boxes)\n",
    "\n",
    "            r = len(boxes)\n",
    "            for i, x in enumerate(zip_lists):                \n",
    "                class_name = list(class_map.keys())[int(x[0])]\n",
    "                score = round(float(x[1]), 2)\n",
    "\n",
    "                x, y, x2, y2 = x[2].cpu().numpy().astype(\"int\")\n",
    "                w = x2 - x\n",
    "                h = y2 - y\n",
    "\n",
    "                s = ' '.join(str(i) for i in [class_name, score, x, y, w, h])\n",
    "\n",
    "                if i<r-1:\n",
    "                    s += '\\n'\n",
    "\n",
    "                file.writelines(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    model_1 = torch.load('training/1/weights/best.pt')\n",
    "    test(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    model_2 = torch.load('training/2/weights/best.pt')\n",
    "    test(model_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd481019de055d53831c4816760b4099e1ec54cf808b21949a4b525a6c74a9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
