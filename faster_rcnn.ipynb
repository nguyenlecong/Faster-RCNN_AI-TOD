{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/yolov5/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules import utils\n",
    "from modules import transforms as T\n",
    "from modules.engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cython\n",
    "# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/pytorch/vision.git\n",
    "\n",
    "# !cp vision/references/detection/utils.py .\n",
    "# !cp vision/references/detection/transforms.py .\n",
    "# !cp vision/references/detection/coco_eval.py .\n",
    "# !cp vision/references/detection/engine.py .\n",
    "# !cp vision/references/detection/coco_utils.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = config.min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = config.class_map\n",
    "num_classes = len(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = torchvision.models.vgg16(\n",
    "                    weights=\"VGG16_Weights.IMAGENET1K_V1\").features\n",
    "backbone.out_channels = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_size = config.anchor_size\n",
    "anchor_ratio = config.anchor_ratio\n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=(anchor_size,),\n",
    "                                   aspect_ratios=(anchor_ratio,))\n",
    "\n",
    "# if rpn_anchor_generator is None:\n",
    "#     rpn_anchor_generator = _default_anchorgen()\n",
    "\n",
    "# def _default_anchorgen():\n",
    "#     anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "#     aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "#     return AnchorGenerator(anchor_sizes, aspect_ratios)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# if box_roi_pool is None:\n",
    "#     box_roi_pool = MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FasterRCNN(backbone=backbone,\n",
    "                   num_classes=num_classes,\n",
    "                   min_size=min_size,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(600,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# model.eval()\n",
    "# x = [torch.rand(3, 300, 400)]\n",
    "# predictions = model(x)\n",
    "\n",
    "# make_dot(predictions[0]['boxes'], params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))  # Follow Fast R-CNN paper\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        self.labels = list(sorted(os.listdir(os.path.join(root, \"labels\"))))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        label_path = os.path.join(self.root, \"labels\", self.labels[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(label_path) as file:\n",
    "            label = [line.rstrip() for line in file]\n",
    "\n",
    "        num_objs = len(label)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in range(num_objs):\n",
    "            xmin, ymin, xmax, ymax, cls = [int(j) for j in label[i].split(', ')]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(cls)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 157, 79)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratio = config.train_ratio\n",
    "val_ratio = config.val_ratio\n",
    "train_val_batch = config.train_val_batch\n",
    "test_batch = config.test_batch\n",
    "\n",
    "train_dataset = CustomDataset(root, get_transform(train=True))\n",
    "val_test_dataset = CustomDataset(root, get_transform(train=False))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "train_num = round(train_ratio*len(train_dataset))\n",
    "val_num = round(val_ratio*len(train_dataset))\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, indices[:train_num])\n",
    "val_dataset = torch.utils.data.Subset(val_test_dataset, indices[train_num:train_num+val_num])\n",
    "test_dataset = torch.utils.data.Subset(val_test_dataset, indices[train_num+val_num:])\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_val_batch,\n",
    "                                          shuffle=True, num_workers=4,\n",
    "                                          collate_fn=utils.collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=train_val_batch,\n",
    "                                               shuffle=False, num_workers=4,\n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mode = config.mode=='train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer - SGD follow Faster R-CNN paper\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # and a learning rate scheduler which decreases the learning rate by\n",
    "    # 10x every 3 epochs\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/550]  eta: 0:39:51  lr: 0.000014  loss: 2.5637 (2.5637)  loss_classifier: 1.3563 (1.3563)  loss_box_reg: 0.0008 (0.0008)  loss_objectness: 0.6944 (0.6944)  loss_rpn_box_reg: 0.5122 (0.5122)  time: 4.3491  data: 0.7145  max mem: 1064\n",
      "Epoch: [0]  [ 10/550]  eta: 0:04:56  lr: 0.000105  loss: 2.7970 (3.3905)  loss_classifier: 1.3420 (1.3417)  loss_box_reg: 0.0045 (0.0107)  loss_objectness: 0.6954 (0.7009)  loss_rpn_box_reg: 0.7607 (1.3373)  time: 0.5482  data: 0.1073  max mem: 1459\n",
      "Epoch: [0]  [ 20/550]  eta: 0:03:24  lr: 0.000196  loss: 3.6558 (3.6484)  loss_classifier: 1.3238 (1.2953)  loss_box_reg: 0.0101 (0.0106)  loss_objectness: 0.6992 (0.7010)  loss_rpn_box_reg: 1.8417 (1.6414)  time: 0.1872  data: 0.0612  max mem: 1757\n",
      "Epoch: [0]  [ 30/550]  eta: 0:02:44  lr: 0.000287  loss: 3.2579 (3.3482)  loss_classifier: 1.1844 (1.2251)  loss_box_reg: 0.0047 (0.0103)  loss_objectness: 0.6928 (0.6946)  loss_rpn_box_reg: 1.4664 (1.4182)  time: 0.1876  data: 0.0556  max mem: 1757\n",
      "Epoch: [0]  [ 40/550]  eta: 0:02:29  lr: 0.000378  loss: 2.3145 (3.0873)  loss_classifier: 1.0066 (1.1039)  loss_box_reg: 0.0069 (0.0230)  loss_objectness: 0.6574 (0.6685)  loss_rpn_box_reg: 0.5354 (1.2919)  time: 0.1949  data: 0.0560  max mem: 1757\n",
      "Epoch: [0]  [ 50/550]  eta: 0:02:16  lr: 0.000469  loss: 2.0642 (2.8943)  loss_classifier: 0.6140 (1.0132)  loss_box_reg: 0.0428 (0.0290)  loss_objectness: 0.5471 (0.6275)  loss_rpn_box_reg: 0.8335 (1.2247)  time: 0.2060  data: 0.0696  max mem: 1757\n",
      "Epoch: [0]  [ 60/550]  eta: 0:02:07  lr: 0.000560  loss: 1.9727 (2.7113)  loss_classifier: 0.6140 (0.9389)  loss_box_reg: 0.0433 (0.0321)  loss_objectness: 0.4000 (0.5905)  loss_rpn_box_reg: 0.8314 (1.1499)  time: 0.1935  data: 0.0611  max mem: 1757\n",
      "Epoch: [0]  [ 70/550]  eta: 0:02:01  lr: 0.000651  loss: 1.8246 (2.5991)  loss_classifier: 0.5793 (0.8761)  loss_box_reg: 0.0300 (0.0335)  loss_objectness: 0.3967 (0.5600)  loss_rpn_box_reg: 0.8113 (1.1294)  time: 0.2033  data: 0.0672  max mem: 1757\n",
      "Epoch: [0]  [ 80/550]  eta: 0:01:56  lr: 0.000742  loss: 2.0677 (2.5620)  loss_classifier: 0.6413 (0.8483)  loss_box_reg: 0.0146 (0.0316)  loss_objectness: 0.4889 (0.5558)  loss_rpn_box_reg: 0.8779 (1.1263)  time: 0.2096  data: 0.0747  max mem: 1757\n",
      "Epoch: [0]  [ 90/550]  eta: 0:01:51  lr: 0.000833  loss: 2.0517 (2.4752)  loss_classifier: 0.6653 (0.8132)  loss_box_reg: 0.0160 (0.0320)  loss_objectness: 0.4523 (0.5307)  loss_rpn_box_reg: 0.9755 (1.0994)  time: 0.2052  data: 0.0720  max mem: 1757\n",
      "Epoch: [0]  [100/550]  eta: 0:01:46  lr: 0.000924  loss: 1.7879 (2.3945)  loss_classifier: 0.5243 (0.7829)  loss_box_reg: 0.0265 (0.0319)  loss_objectness: 0.2722 (0.5051)  loss_rpn_box_reg: 0.9683 (1.0747)  time: 0.1978  data: 0.0632  max mem: 1757\n",
      "Epoch: [0]  [110/550]  eta: 0:01:43  lr: 0.001015  loss: 1.6547 (2.3246)  loss_classifier: 0.5532 (0.7637)  loss_box_reg: 0.0265 (0.0313)  loss_objectness: 0.2527 (0.4807)  loss_rpn_box_reg: 0.8754 (1.0490)  time: 0.1999  data: 0.0610  max mem: 1757\n",
      "Epoch: [0]  [120/550]  eta: 0:01:39  lr: 0.001106  loss: 1.2577 (2.2257)  loss_classifier: 0.4529 (0.7297)  loss_box_reg: 0.0102 (0.0297)  loss_objectness: 0.2534 (0.4623)  loss_rpn_box_reg: 0.6117 (1.0040)  time: 0.2014  data: 0.0645  max mem: 1757\n",
      "Epoch: [0]  [130/550]  eta: 0:01:36  lr: 0.001197  loss: 1.2024 (2.1565)  loss_classifier: 0.4548 (0.7075)  loss_box_reg: 0.0090 (0.0287)  loss_objectness: 0.2260 (0.4410)  loss_rpn_box_reg: 0.5758 (0.9794)  time: 0.2067  data: 0.0708  max mem: 1757\n",
      "Epoch: [0]  [140/550]  eta: 0:01:33  lr: 0.001288  loss: 1.3176 (2.0900)  loss_classifier: 0.4866 (0.6907)  loss_box_reg: 0.0189 (0.0279)  loss_objectness: 0.1537 (0.4216)  loss_rpn_box_reg: 0.6466 (0.9497)  time: 0.1974  data: 0.0611  max mem: 1757\n",
      "Epoch: [0]  [150/550]  eta: 0:01:29  lr: 0.001379  loss: 1.3182 (2.0331)  loss_classifier: 0.4388 (0.6741)  loss_box_reg: 0.0202 (0.0284)  loss_objectness: 0.1537 (0.4041)  loss_rpn_box_reg: 0.6410 (0.9264)  time: 0.1792  data: 0.0480  max mem: 1757\n",
      "Epoch: [0]  [160/550]  eta: 0:01:25  lr: 0.001470  loss: 1.3248 (1.9784)  loss_classifier: 0.4024 (0.6533)  loss_box_reg: 0.0300 (0.0288)  loss_objectness: 0.1727 (0.3900)  loss_rpn_box_reg: 0.6410 (0.9064)  time: 0.1745  data: 0.0437  max mem: 1757\n",
      "Epoch: [0]  [170/550]  eta: 0:01:23  lr: 0.001561  loss: 1.2639 (1.9244)  loss_classifier: 0.2981 (0.6346)  loss_box_reg: 0.0179 (0.0294)  loss_objectness: 0.1274 (0.3738)  loss_rpn_box_reg: 0.6768 (0.8865)  time: 0.1787  data: 0.0432  max mem: 1757\n",
      "Epoch: [0]  [180/550]  eta: 0:01:20  lr: 0.001652  loss: 0.9919 (1.8892)  loss_classifier: 0.4289 (0.6288)  loss_box_reg: 0.0136 (0.0300)  loss_objectness: 0.1274 (0.3623)  loss_rpn_box_reg: 0.3473 (0.8681)  time: 0.1975  data: 0.0616  max mem: 1757\n",
      "Epoch: [0]  [190/550]  eta: 0:01:18  lr: 0.001743  loss: 1.4340 (1.8828)  loss_classifier: 0.4727 (0.6214)  loss_box_reg: 0.0186 (0.0304)  loss_objectness: 0.1689 (0.3534)  loss_rpn_box_reg: 0.5786 (0.8776)  time: 0.2034  data: 0.0679  max mem: 1757\n",
      "Epoch: [0]  [200/550]  eta: 0:01:15  lr: 0.001834  loss: 1.7928 (1.8622)  loss_classifier: 0.5283 (0.6147)  loss_box_reg: 0.0186 (0.0309)  loss_objectness: 0.1877 (0.3455)  loss_rpn_box_reg: 0.9030 (0.8711)  time: 0.2076  data: 0.0785  max mem: 1757\n",
      "Epoch: [0]  [210/550]  eta: 0:01:13  lr: 0.001925  loss: 1.5371 (1.8351)  loss_classifier: 0.4759 (0.6038)  loss_box_reg: 0.0330 (0.0314)  loss_objectness: 0.1734 (0.3385)  loss_rpn_box_reg: 0.7311 (0.8614)  time: 0.2058  data: 0.0746  max mem: 1757\n",
      "Epoch: [0]  [220/550]  eta: 0:01:10  lr: 0.002016  loss: 1.7580 (1.8329)  loss_classifier: 0.5282 (0.5995)  loss_box_reg: 0.0123 (0.0307)  loss_objectness: 0.1737 (0.3310)  loss_rpn_box_reg: 0.9659 (0.8717)  time: 0.1870  data: 0.0531  max mem: 1757\n",
      "Epoch: [0]  [230/550]  eta: 0:01:08  lr: 0.002107  loss: 1.7580 (1.8163)  loss_classifier: 0.5733 (0.5951)  loss_box_reg: 0.0049 (0.0296)  loss_objectness: 0.1865 (0.3261)  loss_rpn_box_reg: 0.9947 (0.8655)  time: 0.1805  data: 0.0536  max mem: 1757\n",
      "Epoch: [0]  [240/550]  eta: 0:01:05  lr: 0.002198  loss: 0.9334 (1.7812)  loss_classifier: 0.3619 (0.5859)  loss_box_reg: 0.0073 (0.0291)  loss_objectness: 0.1888 (0.3197)  loss_rpn_box_reg: 0.2691 (0.8465)  time: 0.1815  data: 0.0521  max mem: 1757\n",
      "Epoch: [0]  [250/550]  eta: 0:01:03  lr: 0.002289  loss: 1.1182 (1.7592)  loss_classifier: 0.4169 (0.5775)  loss_box_reg: 0.0197 (0.0292)  loss_objectness: 0.1696 (0.3140)  loss_rpn_box_reg: 0.4200 (0.8386)  time: 0.1790  data: 0.0481  max mem: 1757\n",
      "Epoch: [0]  [260/550]  eta: 0:01:00  lr: 0.002380  loss: 1.1182 (1.7389)  loss_classifier: 0.4169 (0.5679)  loss_box_reg: 0.0130 (0.0289)  loss_objectness: 0.2035 (0.3105)  loss_rpn_box_reg: 0.4956 (0.8316)  time: 0.1881  data: 0.0516  max mem: 1777\n",
      "Epoch: [0]  [270/550]  eta: 0:00:58  lr: 0.002471  loss: 1.3313 (1.7442)  loss_classifier: 0.4744 (0.5710)  loss_box_reg: 0.0130 (0.0288)  loss_objectness: 0.2193 (0.3071)  loss_rpn_box_reg: 0.6208 (0.8373)  time: 0.2139  data: 0.0693  max mem: 1777\n",
      "Epoch: [0]  [280/550]  eta: 0:00:56  lr: 0.002562  loss: 1.7756 (1.7352)  loss_classifier: 0.6277 (0.5702)  loss_box_reg: 0.0059 (0.0280)  loss_objectness: 0.1633 (0.3016)  loss_rpn_box_reg: 0.9939 (0.8354)  time: 0.2097  data: 0.0684  max mem: 1777\n",
      "Epoch: [0]  [290/550]  eta: 0:00:54  lr: 0.002653  loss: 1.5047 (1.7213)  loss_classifier: 0.5195 (0.5695)  loss_box_reg: 0.0028 (0.0272)  loss_objectness: 0.1549 (0.2972)  loss_rpn_box_reg: 0.6926 (0.8274)  time: 0.2012  data: 0.0657  max mem: 1777\n",
      "Epoch: [0]  [300/550]  eta: 0:00:52  lr: 0.002744  loss: 1.3620 (1.7068)  loss_classifier: 0.5682 (0.5683)  loss_box_reg: 0.0014 (0.0263)  loss_objectness: 0.1328 (0.2915)  loss_rpn_box_reg: 0.6472 (0.8207)  time: 0.2030  data: 0.0656  max mem: 1892\n",
      "Epoch: [0]  [310/550]  eta: 0:00:50  lr: 0.002835  loss: 1.3242 (1.6881)  loss_classifier: 0.5607 (0.5648)  loss_box_reg: 0.0014 (0.0256)  loss_objectness: 0.1321 (0.2877)  loss_rpn_box_reg: 0.6472 (0.8101)  time: 0.1984  data: 0.0577  max mem: 2072\n",
      "Epoch: [0]  [320/550]  eta: 0:00:48  lr: 0.002926  loss: 1.2147 (1.6713)  loss_classifier: 0.5113 (0.5621)  loss_box_reg: 0.0010 (0.0249)  loss_objectness: 0.1389 (0.2836)  loss_rpn_box_reg: 0.4923 (0.8007)  time: 0.2040  data: 0.0639  max mem: 2072\n",
      "Epoch: [0]  [330/550]  eta: 0:00:46  lr: 0.003017  loss: 1.0864 (1.6533)  loss_classifier: 0.5050 (0.5584)  loss_box_reg: 0.0012 (0.0243)  loss_objectness: 0.1389 (0.2797)  loss_rpn_box_reg: 0.4923 (0.7910)  time: 0.2128  data: 0.0759  max mem: 2072\n",
      "Epoch: [0]  [340/550]  eta: 0:00:43  lr: 0.003108  loss: 0.9354 (1.6265)  loss_classifier: 0.3691 (0.5497)  loss_box_reg: 0.0014 (0.0237)  loss_objectness: 0.1443 (0.2756)  loss_rpn_box_reg: 0.3829 (0.7775)  time: 0.2057  data: 0.0651  max mem: 2072\n",
      "Epoch: [0]  [350/550]  eta: 0:00:41  lr: 0.003199  loss: 0.6528 (1.6077)  loss_classifier: 0.3011 (0.5451)  loss_box_reg: 0.0028 (0.0231)  loss_objectness: 0.1334 (0.2716)  loss_rpn_box_reg: 0.2266 (0.7679)  time: 0.1850  data: 0.0466  max mem: 2072\n",
      "Epoch: [0]  [360/550]  eta: 0:00:39  lr: 0.003290  loss: 0.7791 (1.5922)  loss_classifier: 0.3417 (0.5401)  loss_box_reg: 0.0013 (0.0225)  loss_objectness: 0.1092 (0.2670)  loss_rpn_box_reg: 0.2697 (0.7626)  time: 0.1913  data: 0.0526  max mem: 2072\n",
      "Epoch: [0]  [370/550]  eta: 0:00:37  lr: 0.003380  loss: 1.1036 (1.5777)  loss_classifier: 0.4701 (0.5353)  loss_box_reg: 0.0005 (0.0219)  loss_objectness: 0.0975 (0.2630)  loss_rpn_box_reg: 0.5034 (0.7574)  time: 0.1959  data: 0.0530  max mem: 2072\n",
      "Epoch: [0]  [380/550]  eta: 0:00:35  lr: 0.003471  loss: 0.9461 (1.5658)  loss_classifier: 0.3031 (0.5315)  loss_box_reg: 0.0003 (0.0214)  loss_objectness: 0.1427 (0.2604)  loss_rpn_box_reg: 0.5179 (0.7525)  time: 0.1958  data: 0.0532  max mem: 2072\n",
      "Epoch: [0]  [390/550]  eta: 0:00:33  lr: 0.003562  loss: 1.2626 (1.5523)  loss_classifier: 0.3244 (0.5277)  loss_box_reg: 0.0005 (0.0209)  loss_objectness: 0.1406 (0.2572)  loss_rpn_box_reg: 0.5739 (0.7465)  time: 0.1956  data: 0.0566  max mem: 2072\n",
      "Epoch: [0]  [400/550]  eta: 0:00:31  lr: 0.003653  loss: 1.2626 (1.5442)  loss_classifier: 0.4920 (0.5262)  loss_box_reg: 0.0007 (0.0204)  loss_objectness: 0.1315 (0.2547)  loss_rpn_box_reg: 0.5739 (0.7429)  time: 0.2141  data: 0.0799  max mem: 2072\n",
      "Epoch: [0]  [410/550]  eta: 0:00:29  lr: 0.003744  loss: 1.0754 (1.5274)  loss_classifier: 0.4527 (0.5213)  loss_box_reg: 0.0007 (0.0200)  loss_objectness: 0.1315 (0.2517)  loss_rpn_box_reg: 0.4550 (0.7344)  time: 0.2195  data: 0.0814  max mem: 2072\n",
      "Epoch: [0]  [420/550]  eta: 0:00:26  lr: 0.003835  loss: 0.3936 (1.5069)  loss_classifier: 0.1582 (0.5154)  loss_box_reg: 0.0001 (0.0196)  loss_objectness: 0.1170 (0.2487)  loss_rpn_box_reg: 0.1005 (0.7232)  time: 0.1927  data: 0.0541  max mem: 2072\n",
      "Epoch: [0]  [430/550]  eta: 0:00:24  lr: 0.003926  loss: 0.9402 (1.4948)  loss_classifier: 0.3724 (0.5128)  loss_box_reg: 0.0007 (0.0192)  loss_objectness: 0.1170 (0.2460)  loss_rpn_box_reg: 0.4216 (0.7168)  time: 0.1950  data: 0.0598  max mem: 2072\n",
      "Epoch: [0]  [440/550]  eta: 0:00:22  lr: 0.004017  loss: 1.1591 (1.4879)  loss_classifier: 0.4117 (0.5097)  loss_box_reg: 0.0010 (0.0188)  loss_objectness: 0.1252 (0.2434)  loss_rpn_box_reg: 0.5601 (0.7160)  time: 0.2132  data: 0.0776  max mem: 2072\n",
      "Epoch: [0]  [450/550]  eta: 0:00:20  lr: 0.004108  loss: 1.1591 (1.4699)  loss_classifier: 0.3068 (0.5040)  loss_box_reg: 0.0010 (0.0184)  loss_objectness: 0.1252 (0.2409)  loss_rpn_box_reg: 0.5774 (0.7066)  time: 0.2068  data: 0.0669  max mem: 2072\n",
      "Epoch: [0]  [460/550]  eta: 0:00:18  lr: 0.004199  loss: 0.6602 (1.4606)  loss_classifier: 0.3133 (0.5054)  loss_box_reg: 0.0008 (0.0181)  loss_objectness: 0.1106 (0.2381)  loss_rpn_box_reg: 0.1860 (0.6991)  time: 0.2118  data: 0.0619  max mem: 2072\n",
      "Epoch: [0]  [470/550]  eta: 0:00:16  lr: 0.004290  loss: 1.2556 (1.4558)  loss_classifier: 0.5174 (0.5053)  loss_box_reg: 0.0011 (0.0177)  loss_objectness: 0.1003 (0.2354)  loss_rpn_box_reg: 0.5111 (0.6975)  time: 0.2114  data: 0.0625  max mem: 2072\n",
      "Epoch: [0]  [480/550]  eta: 0:00:14  lr: 0.004381  loss: 1.1973 (1.4441)  loss_classifier: 0.4792 (0.5009)  loss_box_reg: 0.0012 (0.0175)  loss_objectness: 0.1048 (0.2326)  loss_rpn_box_reg: 0.6322 (0.6931)  time: 0.2033  data: 0.0603  max mem: 2072\n",
      "Epoch: [0]  [490/550]  eta: 0:00:12  lr: 0.004472  loss: 0.9897 (1.4392)  loss_classifier: 0.3289 (0.4979)  loss_box_reg: 0.0018 (0.0173)  loss_objectness: 0.1146 (0.2312)  loss_rpn_box_reg: 0.2604 (0.6929)  time: 0.2023  data: 0.0604  max mem: 2072\n",
      "Epoch: [0]  [500/550]  eta: 0:00:10  lr: 0.004563  loss: 0.9503 (1.4288)  loss_classifier: 0.3738 (0.4954)  loss_box_reg: 0.0009 (0.0170)  loss_objectness: 0.1111 (0.2286)  loss_rpn_box_reg: 0.4033 (0.6878)  time: 0.1943  data: 0.0484  max mem: 2072\n",
      "Epoch: [0]  [510/550]  eta: 0:00:08  lr: 0.004654  loss: 0.9304 (1.4155)  loss_classifier: 0.3322 (0.4910)  loss_box_reg: 0.0003 (0.0167)  loss_objectness: 0.1452 (0.2272)  loss_rpn_box_reg: 0.4033 (0.6806)  time: 0.2048  data: 0.0604  max mem: 2072\n",
      "Epoch: [0]  [520/550]  eta: 0:00:06  lr: 0.004745  loss: 0.7053 (1.4101)  loss_classifier: 0.2612 (0.4886)  loss_box_reg: 0.0004 (0.0164)  loss_objectness: 0.1452 (0.2252)  loss_rpn_box_reg: 0.3068 (0.6798)  time: 0.2127  data: 0.0729  max mem: 2072\n",
      "Epoch: [0]  [530/550]  eta: 0:00:04  lr: 0.004836  loss: 1.1280 (1.4023)  loss_classifier: 0.4040 (0.4874)  loss_box_reg: 0.0007 (0.0161)  loss_objectness: 0.0991 (0.2228)  loss_rpn_box_reg: 0.6143 (0.6759)  time: 0.2207  data: 0.0810  max mem: 2072\n",
      "Epoch: [0]  [540/550]  eta: 0:00:02  lr: 0.004927  loss: 1.1122 (1.3986)  loss_classifier: 0.4149 (0.4876)  loss_box_reg: 0.0009 (0.0159)  loss_objectness: 0.1077 (0.2217)  loss_rpn_box_reg: 0.4285 (0.6733)  time: 0.2144  data: 0.0755  max mem: 2072\n",
      "Epoch: [0]  [549/550]  eta: 0:00:00  lr: 0.005000  loss: 1.0590 (1.3928)  loss_classifier: 0.4253 (0.4858)  loss_box_reg: 0.0037 (0.0158)  loss_objectness: 0.1480 (0.2205)  loss_rpn_box_reg: 0.4847 (0.6707)  time: 0.1795  data: 0.0481  max mem: 2072\n",
      "Epoch: [0] Total time: 0:01:53 (0.2067 s / it)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/yolov5/lib/python3.8/site-packages/numpy/lib/arraysetops.py:272: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  ar = np.asanyarray(ar)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Results do not correspond to current coco set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.105/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.105/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# evaluate on the test dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.0.105/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/faster_rcnn.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m evaluate(model, val_data_loader, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m/opt/anaconda/envs/yolov5/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/modules/engine.py:106\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m    103\u001b[0m res \u001b[39m=\u001b[39m {target[\u001b[39m\"\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m\"\u001b[39m]: output \u001b[39mfor\u001b[39;00m target,\n\u001b[1;32m    104\u001b[0m        output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(targets, outputs)}\n\u001b[1;32m    105\u001b[0m evaluator_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 106\u001b[0m coco_evaluator\u001b[39m.\u001b[39;49mupdate(res)\n\u001b[1;32m    107\u001b[0m evaluator_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m evaluator_time\n\u001b[1;32m    108\u001b[0m metric_logger\u001b[39m.\u001b[39mupdate(model_time\u001b[39m=\u001b[39mmodel_time,\n\u001b[1;32m    109\u001b[0m                      evaluator_time\u001b[39m=\u001b[39mevaluator_time)\n",
      "File \u001b[0;32m/hdd/nguyenlc/ai_training/faster_rcnn/Faster-RCNN_AI-TOD/modules/coco_eval.py:35\u001b[0m, in \u001b[0;36mCocoEvaluator.update\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m     33\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare(predictions, iou_type)\n\u001b[1;32m     34\u001b[0m \u001b[39mwith\u001b[39;00m redirect_stdout(io\u001b[39m.\u001b[39mStringIO()):\n\u001b[0;32m---> 35\u001b[0m     coco_dt \u001b[39m=\u001b[39m COCO\u001b[39m.\u001b[39;49mloadRes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoco_gt, results) \u001b[39mif\u001b[39;00m results \u001b[39melse\u001b[39;00m COCO()\n\u001b[1;32m     36\u001b[0m coco_eval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoco_eval[iou_type]\n\u001b[1;32m     38\u001b[0m coco_eval\u001b[39m.\u001b[39mcocoDt \u001b[39m=\u001b[39m coco_dt\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pycocotools/coco.py:327\u001b[0m, in \u001b[0;36mCOCO.loadRes\u001b[0;34m(self, resFile)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(anns) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mresults in not an array of objects\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    326\u001b[0m annsImgIds \u001b[39m=\u001b[39m [ann[\u001b[39m'\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m ann \u001b[39min\u001b[39;00m anns]\n\u001b[0;32m--> 327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mset\u001b[39m(annsImgIds) \u001b[39m==\u001b[39m (\u001b[39mset\u001b[39m(annsImgIds) \u001b[39m&\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetImgIds())), \\\n\u001b[1;32m    328\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mResults do not correspond to current coco set\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m anns[\u001b[39m0\u001b[39m]:\n\u001b[1;32m    330\u001b[0m     imgIds \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([img[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mdataset[\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m]]) \u001b[39m&\u001b[39m \u001b[39mset\u001b[39m([ann[\u001b[39m'\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m ann \u001b[39min\u001b[39;00m anns])\n",
      "\u001b[0;31mAssertionError\u001b[0m: Results do not correspond to current coco set"
     ]
    }
   ],
   "source": [
    "if train_mode:\n",
    "    num_epochs = config.num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=10)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the test dataset\n",
    "        # evaluate(model, val_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    file_num = len([i for i in os.listdir('weights')])\n",
    "    torch.save(model, f'weights/exp_{file_num}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import color_palette\n",
    "\n",
    "\n",
    "def make_color_map():\n",
    "    '''\n",
    "        Create a color map for each class\n",
    "    '''\n",
    "    names = sorted(set(list(class_map.values())))\n",
    "    n = len(names)\n",
    "    cp = color_palette(\"Paired\", n)\n",
    "\n",
    "    cp[:] = [tuple(int(255*c) for c in rgb) for rgb in cp]\n",
    "\n",
    "    return dict(zip(names, cp))\n",
    "\n",
    "if not train_mode:   \n",
    "    color_map = make_color_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, iou_thresh=0.5):\n",
    "    return torchvision.ops.nms(boxes, scores, iou_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(image, boxes, cls, scr):\n",
    "    color = color_map[cls]\n",
    "\n",
    "    x1, y1, x2, y2 = boxes.cpu().numpy().astype(\"int\")\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "    cv2.putText(image, ' '.join([cls, scr]), (x1, y1-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2,\n",
    "                cv2.LINE_AA)\n",
    "    cv2.putText(image, ' '.join([cls, scr]), (x1, y1-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 1,\n",
    "                cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, img):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model([img.to(device)])\n",
    "\n",
    "    image = img.numpy().transpose(1, 2, 0)\n",
    "    image = (image*255).astype('uint8')\n",
    "\n",
    "    boxes = pred[0]['boxes']\n",
    "    scores = pred[0]['scores']\n",
    "    keep = nms(boxes, scores, 0.1)\n",
    "\n",
    "    labels = pred[0]['labels']\n",
    "\n",
    "    for i in keep:\n",
    "        cls = class_map[int(labels[i])]\n",
    "        scr = str(round(float(scores[i]), 2))\n",
    "        plot_result(image, boxes[i], cls, scr)\n",
    "\n",
    "    image = image[:,:,::-1]\n",
    "    cv2.imwrite('tmp.tif', image)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(image[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # pick one image from the test set\n",
    "    img, _ = test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # load model from weight\n",
    "    model1 = torch.load('weights/exp.pt')\n",
    "    model2 = torch.load('weights/exp_1.pt')\n",
    "    # model2 = torch.load('weights/exp_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # AnchorGenerator(sizes=((64, 128, 256, 512),),\n",
    "    #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    # min_size = 600\n",
    "    infer(model1, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "    #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    # min_size = 1200\n",
    "    infer(model2, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not train_mode:\n",
    "#     # AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n",
    "#     #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "#     # min_size = 1200\n",
    "#     infer(model3, img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
