{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/yolov5/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules import utils\n",
    "from modules import transforms as T\n",
    "from modules.engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cython\n",
    "# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/pytorch/vision.git\n",
    "\n",
    "# !cp vision/references/detection/utils.py .\n",
    "# !cp vision/references/detection/transforms.py .\n",
    "# !cp vision/references/detection/coco_eval.py .\n",
    "# !cp vision/references/detection/engine.py .\n",
    "# !cp vision/references/detection/coco_utils.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = config.min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = config.class_map\n",
    "num_classes = len(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = torchvision.models.vgg16(\n",
    "                    weights=\"VGG16_Weights.IMAGENET1K_V1\").features\n",
    "backbone.out_channels = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_size = config.anchor_size\n",
    "anchor_ratio = config.anchor_ratio\n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=(anchor_size,),\n",
    "                                   aspect_ratios=(anchor_ratio,))\n",
    "\n",
    "# if rpn_anchor_generator is None:\n",
    "#     rpn_anchor_generator = _default_anchorgen()\n",
    "\n",
    "# def _default_anchorgen():\n",
    "#     anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "#     aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "#     return AnchorGenerator(anchor_sizes, aspect_ratios)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# if box_roi_pool is None:\n",
    "#     box_roi_pool = MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FasterRCNN(backbone=backbone,\n",
    "                   num_classes=num_classes,\n",
    "                   min_size=min_size,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(600,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 84, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=8, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# model.eval()\n",
    "# x = [torch.rand(3, 300, 400)]\n",
    "# predictions = model(x)\n",
    "\n",
    "# make_dot(predictions[0]['boxes'], params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))  # Follow Fast R-CNN paper\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.label_root = root.replace('/hdd/thaihq/qnet_search/ori_data', '/hdd/nguyenlc/ai_training/faster_rcnn/qnet_search/correct_labels')\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        # self.labels = list(sorted(os.listdir(os.path.join(self.label_root, \"labels\"))))\n",
    "        self.labels = list(sorted(os.listdir(self.label_root)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        label_path = os.path.join(self.label_root, self.labels[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(label_path) as file:\n",
    "            label = [line.rstrip() for line in file]\n",
    "\n",
    "        num_objs = len(label)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in range(num_objs):\n",
    "            # xmin, ymin, xmax, ymax, cls = [int(j) for j in label[i].split(', ')]\n",
    "            # boxes.append([xmin, ymin, xmax, ymax])\n",
    "            xmin, ymin, width, height, cls = [int(j) for j in label[i].split(', ')]\n",
    "            boxes.append([xmin, ymin, xmin+width, ymin+height])\n",
    "            labels.append(cls-1)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = config.train_ratio\n",
    "# val_ratio = config.val_ratio\n",
    "# train_val_batch = config.train_val_batch\n",
    "# test_batch = config.test_batch\n",
    "\n",
    "# train_dataset = CustomDataset(root, get_transform(train=True))\n",
    "# val_test_dataset = CustomDataset(root, get_transform(train=False))\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "# train_num = round(train_ratio*len(train_dataset))\n",
    "# val_num = round(val_ratio*len(train_dataset))\n",
    "# indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, indices[:train_num])\n",
    "# val_dataset = torch.utils.data.Subset(val_test_dataset, indices[train_num:train_num+val_num])\n",
    "# test_dataset = torch.utils.data.Subset(val_test_dataset, indices[train_num+val_num:])\n",
    "\n",
    "# train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_val_batch,\n",
    "#                                           shuffle=True, num_workers=4,\n",
    "#                                           collate_fn=utils.collate_fn)\n",
    "\n",
    "# val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=train_val_batch,\n",
    "#                                                shuffle=False, num_workers=4,\n",
    "#                                                collate_fn=utils.collate_fn)\n",
    "\n",
    "# len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11214, 2804, 14018)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_batch = config.train_val_batch\n",
    "test_batch = config.test_batch\n",
    "\n",
    "train_dataset = CustomDataset('/hdd/thaihq/qnet_search/ori_data/train', get_transform(train=True))\n",
    "val_dataset = CustomDataset('/hdd/thaihq/qnet_search/ori_data/val', get_transform(train=False))\n",
    "test_dataset = CustomDataset('/hdd/thaihq/qnet_search/ori_data/test', get_transform(train=False))\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_val_batch,\n",
    "                                          shuffle=True, num_workers=4,\n",
    "                                          collate_fn=utils.collate_fn)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=train_val_batch,\n",
    "                                               shuffle=False, num_workers=4,\n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=train_val_batch,\n",
    "                                               shuffle=False, num_workers=4,\n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mode = config.mode=='train'\n",
    "train_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_mode:\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer - SGD follow Faster R-CNN paper\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=config.learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # and a learning rate scheduler which decreases the learning rate by\n",
    "    # 10x every 3 epochs\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/701]  eta: 0:32:40  lr: 0.000002  loss: 3.3564 (3.3564)  loss_classifier: 2.0721 (2.0721)  loss_box_reg: 0.0115 (0.0115)  loss_objectness: 0.6935 (0.6935)  loss_rpn_box_reg: 0.5794 (0.5794)  time: 2.7968  data: 0.4067  max mem: 2780\n",
      "Epoch: [0]  [100/701]  eta: 0:02:16  lr: 0.000145  loss: 1.4126 (2.4665)  loss_classifier: 0.5915 (1.5478)  loss_box_reg: 0.0123 (0.0114)  loss_objectness: 0.6734 (0.6880)  loss_rpn_box_reg: 0.1750 (0.2193)  time: 0.2031  data: 0.0093  max mem: 3111\n",
      "Epoch: [0]  [200/701]  eta: 0:01:48  lr: 0.000288  loss: 0.4024 (1.7335)  loss_classifier: 0.1510 (0.9404)  loss_box_reg: 0.0113 (0.0219)  loss_objectness: 0.2439 (0.5667)  loss_rpn_box_reg: 0.0550 (0.2045)  time: 0.2086  data: 0.0092  max mem: 3111\n",
      "Epoch: [0]  [300/701]  eta: 0:01:25  lr: 0.000431  loss: 0.1807 (1.3680)  loss_classifier: 0.0688 (0.6983)  loss_box_reg: 0.0045 (0.0275)  loss_objectness: 0.1072 (0.4517)  loss_rpn_box_reg: 0.0231 (0.1905)  time: 0.2037  data: 0.0107  max mem: 3111\n",
      "Epoch: [0]  [400/701]  eta: 0:01:03  lr: 0.000573  loss: 0.0766 (1.0830)  loss_classifier: 0.0251 (0.5425)  loss_box_reg: 0.0009 (0.0254)  loss_objectness: 0.0406 (0.3599)  loss_rpn_box_reg: 0.0116 (0.1552)  time: 0.2251  data: 0.0098  max mem: 3111\n",
      "Epoch: [0]  [500/701]  eta: 0:00:42  lr: 0.000716  loss: 0.4849 (0.9363)  loss_classifier: 0.1827 (0.4610)  loss_box_reg: 0.0149 (0.0243)  loss_objectness: 0.1627 (0.3097)  loss_rpn_box_reg: 0.1169 (0.1413)  time: 0.2003  data: 0.0090  max mem: 3111\n",
      "Epoch: [0]  [600/701]  eta: 0:00:21  lr: 0.000859  loss: 0.3533 (0.8762)  loss_classifier: 0.1354 (0.4230)  loss_box_reg: 0.0066 (0.0264)  loss_objectness: 0.1301 (0.2851)  loss_rpn_box_reg: 0.0604 (0.1417)  time: 0.2039  data: 0.0096  max mem: 3111\n",
      "Epoch: [0]  [700/701]  eta: 0:00:00  lr: 0.001000  loss: 0.0515 (0.7619)  loss_classifier: 0.0174 (0.3664)  loss_box_reg: 0.0013 (0.0228)  loss_objectness: 0.0254 (0.2495)  loss_rpn_box_reg: 0.0070 (0.1232)  time: 0.2041  data: 0.0093  max mem: 3111\n",
      "Epoch: [0] Total time: 0:02:26 (0.2088 s / it)\n",
      "Epoch: [1]  [  0/701]  eta: 0:07:37  lr: 0.001000  loss: 1.1606 (1.1606)  loss_classifier: 0.5639 (0.5639)  loss_box_reg: 0.0470 (0.0470)  loss_objectness: 0.2565 (0.2565)  loss_rpn_box_reg: 0.2932 (0.2932)  time: 0.6525  data: 0.4382  max mem: 3111\n",
      "Epoch: [1]  [100/701]  eta: 0:02:04  lr: 0.001000  loss: 0.3693 (0.4295)  loss_classifier: 0.1456 (0.1795)  loss_box_reg: 0.0175 (0.0188)  loss_objectness: 0.1204 (0.1208)  loss_rpn_box_reg: 0.0819 (0.1106)  time: 0.1995  data: 0.0091  max mem: 3111\n",
      "Epoch: [1]  [200/701]  eta: 0:01:43  lr: 0.001000  loss: 0.1766 (0.4605)  loss_classifier: 0.0704 (0.1885)  loss_box_reg: 0.0171 (0.0283)  loss_objectness: 0.0539 (0.1263)  loss_rpn_box_reg: 0.0415 (0.1174)  time: 0.2125  data: 0.0128  max mem: 3111\n",
      "Epoch: [1]  [300/701]  eta: 0:01:23  lr: 0.001000  loss: 0.1093 (0.4772)  loss_classifier: 0.0448 (0.1943)  loss_box_reg: 0.0035 (0.0301)  loss_objectness: 0.0449 (0.1304)  loss_rpn_box_reg: 0.0191 (0.1224)  time: 0.2164  data: 0.0099  max mem: 3111\n",
      "Epoch: [1]  [400/701]  eta: 0:01:02  lr: 0.001000  loss: 0.0554 (0.4043)  loss_classifier: 0.0209 (0.1640)  loss_box_reg: 0.0024 (0.0263)  loss_objectness: 0.0236 (0.1123)  loss_rpn_box_reg: 0.0091 (0.1019)  time: 0.2103  data: 0.0126  max mem: 3111\n",
      "Epoch: [1]  [500/701]  eta: 0:00:41  lr: 0.001000  loss: 0.4288 (0.3857)  loss_classifier: 0.1795 (0.1586)  loss_box_reg: 0.0144 (0.0247)  loss_objectness: 0.1358 (0.1065)  loss_rpn_box_reg: 0.0956 (0.0959)  time: 0.2063  data: 0.0108  max mem: 3111\n",
      "Epoch: [1]  [600/701]  eta: 0:00:20  lr: 0.001000  loss: 0.3292 (0.3979)  loss_classifier: 0.1139 (0.1649)  loss_box_reg: 0.0075 (0.0237)  loss_objectness: 0.1091 (0.1088)  loss_rpn_box_reg: 0.0457 (0.1004)  time: 0.2224  data: 0.0132  max mem: 3111\n",
      "Epoch: [1]  [700/701]  eta: 0:00:00  lr: 0.001000  loss: 0.0436 (0.3489)  loss_classifier: 0.0154 (0.1444)  loss_box_reg: 0.0021 (0.0207)  loss_objectness: 0.0202 (0.0965)  loss_rpn_box_reg: 0.0049 (0.0873)  time: 0.2059  data: 0.0102  max mem: 3111\n",
      "Epoch: [1] Total time: 0:02:24 (0.2066 s / it)\n",
      "Epoch: [2]  [  0/701]  eta: 0:07:45  lr: 0.001000  loss: 1.1018 (1.1018)  loss_classifier: 0.5250 (0.5250)  loss_box_reg: 0.0171 (0.0171)  loss_objectness: 0.2629 (0.2629)  loss_rpn_box_reg: 0.2968 (0.2968)  time: 0.6644  data: 0.4735  max mem: 3111\n",
      "Epoch: [2]  [100/701]  eta: 0:02:08  lr: 0.001000  loss: 0.3574 (0.3907)  loss_classifier: 0.1445 (0.1674)  loss_box_reg: 0.0287 (0.0275)  loss_objectness: 0.0951 (0.1023)  loss_rpn_box_reg: 0.0679 (0.0936)  time: 0.2031  data: 0.0105  max mem: 3111\n",
      "Epoch: [2]  [200/701]  eta: 0:01:45  lr: 0.001000  loss: 0.1651 (0.4185)  loss_classifier: 0.0706 (0.1760)  loss_box_reg: 0.0155 (0.0342)  loss_objectness: 0.0431 (0.1061)  loss_rpn_box_reg: 0.0378 (0.1022)  time: 0.2116  data: 0.0137  max mem: 3111\n",
      "Epoch: [2]  [300/701]  eta: 0:01:24  lr: 0.001000  loss: 0.0879 (0.4306)  loss_classifier: 0.0332 (0.1780)  loss_box_reg: 0.0051 (0.0359)  loss_objectness: 0.0374 (0.1088)  loss_rpn_box_reg: 0.0155 (0.1079)  time: 0.2079  data: 0.0110  max mem: 3111\n",
      "Epoch: [2]  [400/701]  eta: 0:01:03  lr: 0.001000  loss: 0.0524 (0.3640)  loss_classifier: 0.0189 (0.1496)  loss_box_reg: 0.0028 (0.0314)  loss_objectness: 0.0205 (0.0933)  loss_rpn_box_reg: 0.0087 (0.0898)  time: 0.2111  data: 0.0123  max mem: 3111\n",
      "Epoch: [2]  [500/701]  eta: 0:00:41  lr: 0.001000  loss: 0.3778 (0.3501)  loss_classifier: 0.1568 (0.1457)  loss_box_reg: 0.0164 (0.0292)  loss_objectness: 0.1227 (0.0899)  loss_rpn_box_reg: 0.0886 (0.0852)  time: 0.2149  data: 0.0129  max mem: 3111\n",
      "Epoch: [2]  [600/701]  eta: 0:00:21  lr: 0.001000  loss: 0.3126 (0.3632)  loss_classifier: 0.1139 (0.1524)  loss_box_reg: 0.0101 (0.0282)  loss_objectness: 0.1114 (0.0933)  loss_rpn_box_reg: 0.0418 (0.0893)  time: 0.1864  data: 0.0126  max mem: 3111\n",
      "Epoch: [2]  [700/701]  eta: 0:00:00  lr: 0.001000  loss: 0.0397 (0.3185)  loss_classifier: 0.0151 (0.1334)  loss_box_reg: 0.0033 (0.0247)  loss_objectness: 0.0187 (0.0827)  loss_rpn_box_reg: 0.0049 (0.0777)  time: 0.2054  data: 0.0106  max mem: 3111\n",
      "Epoch: [2] Total time: 0:02:25 (0.2077 s / it)\n",
      "Epoch: [3]  [  0/701]  eta: 0:07:49  lr: 0.000100  loss: 1.0869 (1.0869)  loss_classifier: 0.5232 (0.5232)  loss_box_reg: 0.0175 (0.0175)  loss_objectness: 0.2941 (0.2941)  loss_rpn_box_reg: 0.2521 (0.2521)  time: 0.6700  data: 0.4870  max mem: 3111\n",
      "Epoch: [3]  [100/701]  eta: 0:02:04  lr: 0.000100  loss: 0.3460 (0.3807)  loss_classifier: 0.1473 (0.1647)  loss_box_reg: 0.0263 (0.0305)  loss_objectness: 0.0942 (0.0989)  loss_rpn_box_reg: 0.0646 (0.0866)  time: 0.2038  data: 0.0103  max mem: 3111\n",
      "Epoch: [3]  [200/701]  eta: 0:01:42  lr: 0.000100  loss: 0.1594 (0.4077)  loss_classifier: 0.0646 (0.1722)  loss_box_reg: 0.0197 (0.0386)  loss_objectness: 0.0578 (0.1021)  loss_rpn_box_reg: 0.0348 (0.0947)  time: 0.2047  data: 0.0107  max mem: 3111\n",
      "Epoch: [3]  [300/701]  eta: 0:01:22  lr: 0.000100  loss: 0.0798 (0.4171)  loss_classifier: 0.0337 (0.1738)  loss_box_reg: 0.0097 (0.0410)  loss_objectness: 0.0300 (0.1032)  loss_rpn_box_reg: 0.0131 (0.0992)  time: 0.2057  data: 0.0112  max mem: 3111\n",
      "Epoch: [3]  [400/701]  eta: 0:01:01  lr: 0.000100  loss: 0.0486 (0.3519)  loss_classifier: 0.0191 (0.1459)  loss_box_reg: 0.0048 (0.0352)  loss_objectness: 0.0153 (0.0879)  loss_rpn_box_reg: 0.0083 (0.0829)  time: 0.2134  data: 0.0111  max mem: 3111\n",
      "Epoch: [3]  [500/701]  eta: 0:00:40  lr: 0.000100  loss: 0.4104 (0.3404)  loss_classifier: 0.1792 (0.1434)  loss_box_reg: 0.0220 (0.0329)  loss_objectness: 0.1053 (0.0850)  loss_rpn_box_reg: 0.0940 (0.0791)  time: 0.2076  data: 0.0107  max mem: 3111\n",
      "Epoch: [3]  [600/701]  eta: 0:00:20  lr: 0.000100  loss: 0.2134 (0.3500)  loss_classifier: 0.1043 (0.1481)  loss_box_reg: 0.0108 (0.0317)  loss_objectness: 0.0986 (0.0878)  loss_rpn_box_reg: 0.0315 (0.0824)  time: 0.2085  data: 0.0113  max mem: 3111\n",
      "Epoch: [3]  [700/701]  eta: 0:00:00  lr: 0.000100  loss: 0.0453 (0.3070)  loss_classifier: 0.0166 (0.1297)  loss_box_reg: 0.0049 (0.0278)  loss_objectness: 0.0161 (0.0778)  loss_rpn_box_reg: 0.0046 (0.0717)  time: 0.2087  data: 0.0113  max mem: 3111\n",
      "Epoch: [3] Total time: 0:02:23 (0.2043 s / it)\n",
      "Stopping training early as no improvement observed in last 1 epochs.Best results observed at epoch 2\n"
     ]
    }
   ],
   "source": [
    "if train_mode:\n",
    "    from early_stopping import EarlyStopping\n",
    "    stopper = EarlyStopping(config.patience)\n",
    "    log_path = 'training/log.txt'\n",
    "\n",
    "    num_epochs = config.num_epochs\n",
    "\n",
    "    logs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        log = train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=100)\n",
    "        stopper.log(log_path, str(log))\n",
    "\n",
    "        torch.save(model, f'weights/last.pt')\n",
    "\n",
    "        loss = float(str(log).split('  ')[1].split(' ')[2][1:-1])\n",
    "        stop = stopper(epoch, loss)\n",
    "        if stop:\n",
    "            file_num = len([i for i in os.listdir('weights')])\n",
    "            torch.save(model, f'weights/best.pt')\n",
    "            break\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the test dataset\n",
    "        # evaluate(model, val_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(path):\n",
    "    if not os.path.exists(path):\n",
    "        return\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    lines = file.readlines()\n",
    "\n",
    "    loss_1 = [float(str(i).split('  ')[1].split(' ')[1]) for i in lines]\n",
    "    loss_2 = [float(str(i).split('  ')[1].split(' ')[2][1:-1]) for i in lines]\n",
    "\n",
    "    plt.figure(figsize=(15, 10), tight_layout=True)\n",
    "    plt.plot(range(len(loss_1)), loss_1, label='loss_1')\n",
    "    plt.plot(range(len(loss_2)), loss_2, label='loss_2')\n",
    "\n",
    "    min_loss = min(loss_2)\n",
    "    min_index = loss_2.index(min(loss_2))\n",
    "    plt.plot(min_index, min_loss, '*', label='best value')\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'training/loss.png')\n",
    "    plt.show()\n",
    "\n",
    "plot(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(model, test_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import color_palette\n",
    "\n",
    "\n",
    "def make_color_map():\n",
    "    '''\n",
    "        Create a color map for each class\n",
    "    '''\n",
    "    names = sorted(set(list(class_map.keys())))\n",
    "    n = len(names)\n",
    "    cp = color_palette(\"Paired\", n)\n",
    "\n",
    "    cp[:] = [tuple(int(255*c) for c in rgb) for rgb in cp]\n",
    "\n",
    "    return dict(zip(names, cp))\n",
    "\n",
    "if not train_mode:   \n",
    "    color_map = make_color_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, iou_thresh=0.5):\n",
    "    return torchvision.ops.nms(boxes, scores, iou_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(image, boxes, cls, scr):\n",
    "    color = color_map[cls]\n",
    "\n",
    "    x1, y1, x2, y2 = boxes.cpu().numpy().astype(\"int\")\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 1)\n",
    "\n",
    "    cv2.putText(image, ' '.join([cls, scr]), (x1, y1-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1,\n",
    "                cv2.LINE_AA)\n",
    "    # cv2.putText(image, ' '.join([cls, scr]), (x1, y1-10),\n",
    "    #             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 1,\n",
    "    #             cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, img):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model([img.to(device)])\n",
    "\n",
    "    image = img.numpy().transpose(1, 2, 0)\n",
    "    image = (image*255).astype('uint8')\n",
    "\n",
    "    boxes = pred[0]['boxes']\n",
    "    scores = pred[0]['scores']\n",
    "    keep = nms(boxes, scores, 0.1)\n",
    "\n",
    "    labels = pred[0]['labels']\n",
    "\n",
    "    for i in keep:\n",
    "        # cls = class_map[int(labels[i])]\n",
    "        cls = list(class_map.keys())[int(labels[i])]\n",
    "        scr = str(round(float(scores[i]), 2))\n",
    "        plot_result(image, boxes[i], cls, scr)\n",
    "\n",
    "    image = image[:,:,::-1]\n",
    "    cv2.imwrite('tmp.tif', image)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(image[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # pick one image from the test set\n",
    "    img, _ = test_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # load model from weight\n",
    "    # model1 = torch.load('weights/exp.pt')\n",
    "    # model2 = torch.load('weights/exp_1.pt')\n",
    "    # model3 = torch.load('weights/exp_2.pt')\n",
    "    model4 = torch.load('weights/exp_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not train_mode:\n",
    "#     # AnchorGenerator(sizes=((64, 128, 256, 512),),\n",
    "#     #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "#     # min_size = 600\n",
    "#     infer(model1, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not train_mode:\n",
    "#     # AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#     #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "#     # min_size = 1200\n",
    "#     infer(model2, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not train_mode:\n",
    "#     # AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n",
    "#     #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "#     # min_size = 1200\n",
    "#     infer(model3, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_mode:\n",
    "    # AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n",
    "    #                                aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    # min_size = 600\n",
    "    infer(model4, img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afd481019de055d53831c4816760b4099e1ec54cf808b21949a4b525a6c74a9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
